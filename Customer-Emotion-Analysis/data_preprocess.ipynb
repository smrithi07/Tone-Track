{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database: ['business', 'review', 'amazon_reviews', 'sqlite_sequence', 'imdb_reviews']\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the SQLite database\n",
    "db_path = \"yelp_data.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get all table names\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = [table[0] for table in cursor.fetchall()]\n",
    "\n",
    "print(\"Tables in the database:\", tables)\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\smrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\smrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\smrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "# Download necessary NLP resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load spaCy model for lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):  # Ensure input is a string\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    # Remove irrelevant characters (numbers, punctuation)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Apply Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Remove short words (length ≤ 2)\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "\n",
    "    # Convert tokens back to string\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: business\n",
      " Preprocessed and updated business.\n",
      "Processing table: review\n",
      " Preprocessed and updated review.\n",
      "Processing table: amazon_reviews\n",
      " Preprocessed and updated amazon_reviews.\n",
      "Processing table: sqlite_sequence\n",
      " No text column found in sqlite_sequence. Skipping...\n",
      "Processing table: imdb_reviews\n",
      " Preprocessed and updated imdb_reviews.\n",
      " All tables processed!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_all_tables(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Fetch all table names\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = [table[0] for table in cursor.fetchall()]\n",
    "\n",
    "    for table in tables:\n",
    "        print(f\"Processing table: {table}\")\n",
    "\n",
    "        # Get column names for the table\n",
    "        cursor.execute(f\"PRAGMA table_info({table});\")\n",
    "        columns = [col[1] for col in cursor.fetchall()]\n",
    "\n",
    "        # Find the first TEXT column (assuming reviews are stored there)\n",
    "        text_column = None\n",
    "        for col in columns:\n",
    "            cursor.execute(f\"SELECT typeof({col}) FROM {table} LIMIT 5;\")\n",
    "            types = {row[0] for row in cursor.fetchall()}\n",
    "            if \"text\" in types or \"TEXT\" in types:\n",
    "                text_column = col\n",
    "                break\n",
    "\n",
    "        if not text_column:\n",
    "            print(f\" No text column found in {table}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Load data into DataFrame\n",
    "        df = pd.read_sql(f\"SELECT * FROM {table}\", conn)\n",
    "\n",
    "        # Check for missing values and drop rows with missing text\n",
    "        df.dropna(subset=[text_column], inplace=True)\n",
    "\n",
    "        # Apply preprocessing\n",
    "        df['preprocessed_text'] = df[text_column].apply(preprocess_text)\n",
    "\n",
    "        # Save back to the database\n",
    "        df.to_sql(table, conn, if_exists='replace', index=False)\n",
    "\n",
    "        print(f\" Preprocessed and updated {table}.\")\n",
    "\n",
    "    conn.close()\n",
    "    print(\" All tables processed!\")\n",
    "\n",
    "# Run the function\n",
    "preprocess_all_tables(\"yelp_data.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def preview_preprocessed_data(db_path, table_name):\n",
    "    \"\"\"Fetch and display a sample of preprocessed data.\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # Load the data from the given table\n",
    "    df = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    # Show sample data\n",
    "    print(\" Sample Preprocessed Data:\")\n",
    "    print(df[['preprocessed_text']].sample(5))  # Display 5 random rows\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample Preprocessed Data:\n",
      "           preprocessed_text\n",
      "129717    yovumkpubntdyawzvg\n",
      "8591      ptlgcnxyfhmpiyenbq\n",
      "139515    esbkcxfpzytzpatilq\n",
      "23378   wvokxjymnvuasfnvlopw\n",
      "130850   wqiuqccpmfkhxhchgsw\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "db_path = \"yelp_data.db\"\n",
    "table_name = \"business\"  \n",
    "preview_preprocessed_data(db_path, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample Preprocessed Data:\n",
      "          preprocessed_text\n",
      "3979515    bkfyqtaaruqqvgkw\n",
      "2296209  dssmrqbzgrknnkxpew\n",
      "3681452  dbgtgkqfljrixhdgag\n",
      "5540758    wnycovttglrtwbiq\n",
      "964497    hmeefrochrgyzhilq\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "db_path = \"yelp_data.db\"\n",
    "table_name = \"review\"  \n",
    "preview_preprocessed_data(db_path, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample Preprocessed Data:\n",
      "                                       preprocessed_text\n",
      "23376  great documentary life firefighter worst terro...\n",
      "20371  first review saw page said madhur bhandarkar f...\n",
      "7337   well let say always steven seagal fan movie us...\n",
      "9102   movie epitomizes fear even today fear people p...\n",
      "18205  wonderland spoiler july five people ron launiu...\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "db_path = \"yelp_data.db\"\n",
    "table_name = \"imdb_reviews\"  \n",
    "preview_preprocessed_data(db_path, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample Preprocessed Data:\n",
      "                  preprocessed_text\n",
      "2167085                   fan first\n",
      "2813913             great price job\n",
      "977808   polished design unique spa\n",
      "3368766                  misleading\n",
      "2184207               shipping much\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "db_path = \"yelp_data.db\"\n",
    "table_name = \"amazon_reviews\"  \n",
    "preview_preprocessed_data(db_path, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Columns in 'review' table:\n",
      "- review_id\n",
      "- business_id\n",
      "- stars\n",
      "- text\n",
      "- date\n",
      "- preprocessed_text\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to SQLite database\n",
    "db_path = \"yelp_data.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch column names from the review table\n",
    "cursor.execute(\"PRAGMA table_info(review);\")  # Get table schema\n",
    "columns = cursor.fetchall()\n",
    "\n",
    "# Print column names\n",
    "print(\" Columns in 'review' table:\")\n",
    "for col in columns:\n",
    "    print(f\"- {col[1]}\")  # Column name is at index 1\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                review_id  stars  \\\n",
      "0  KU_O5udG6zpxOg-VcAEodg    3.0   \n",
      "1  BiTunyQ73aT9WBnpR9DZGw    5.0   \n",
      "2  saUsX_uimxRlCVr67Z4Jig    3.0   \n",
      "3  AqPFMleE6RsU23_auESxiA    5.0   \n",
      "4  Sx8TMOWLNuJBWer-0pcmoA    4.0   \n",
      "\n",
      "                                                text    preprocessed_text  \n",
      "0  If you decide to eat here, just be aware it is...   kuoudgzpxogvcaeodg  \n",
      "1  I've taken a lot of spin classes over the year...   bitunyqatwbnprdzgw  \n",
      "2  Family diner. Had the buffet. Eclectic assortm...   sausxuimxrlcvrzjig  \n",
      "3  Wow!  Yummy, different,  delicious.   Our favo...   aqpfmleersuauesxia  \n",
      "4  Cute interior and owner (?) gave us tour of up...  sxtmowlnujbwerpcmoa  \n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to SQLite database\n",
    "db_path = \"yelp_data.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Fetch sample data\n",
    "query = \"SELECT review_id, stars, text, preprocessed_text FROM review LIMIT 5;\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Display data\n",
    "print(df)\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Connect to SQLite database\n",
    "db_path = \"yelp_data.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Step 1: Add `preprocessed_text` column if not exists\n",
    "cursor.execute(\"PRAGMA table_info(review);\")\n",
    "columns = [col[1] for col in cursor.fetchall()]\n",
    "if \"preprocessed_text\" not in columns:\n",
    "    cursor.execute(\"ALTER TABLE review ADD COLUMN preprocessed_text TEXT;\")\n",
    "    conn.commit()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    # Remove numbers and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Remove words with length ≤ 2\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "\n",
    "    # Convert list of tokens back to string\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Fetch all reviews from the table\n",
    "cursor.execute(\"SELECT review_id, text FROM review\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# # Step 2: Preprocess and update the database\n",
    "# for review_id, text in rows:\n",
    "#     preprocessed_text = preprocess_text(text)\n",
    "#     cursor.execute(\"UPDATE review SET preprocessed_text = ? WHERE review_id = ?\", (preprocessed_text, review_id))\n",
    "\n",
    "# # Commit changes and close connection\n",
    "# conn.commit()\n",
    "# conn.close()\n",
    "\n",
    "# print(\" Preprocessing complete! `preprocessed_text` column updated in `review` table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. File updated with preprocessed text.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy model (disable unnecessary components for speed)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text by lemmatizing and filtering stopwords.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return \" \".join(token.lemma_ for token in doc if token.is_alpha and not token.is_stop and len(token.text) > 2)\n",
    "\n",
    "# Load the GoEmotions dataset (JSON file)\n",
    "with open('goemotions_cleaned.json', 'r') as file:\n",
    "    goemotions_data = json.load(file)\n",
    "\n",
    "# Convert to DataFrame\n",
    "goemotions_df = pd.DataFrame(goemotions_data)\n",
    "\n",
    "# Apply preprocessing\n",
    "goemotions_df['preprocessed_text'] = goemotions_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Save back to JSON file\n",
    "with open('goemotions_cleaned.json', 'w') as file:\n",
    "    json.dump(goemotions_df.to_dict(orient=\"records\"), file, indent=4)\n",
    "\n",
    "print(\"Preprocessing complete. File updated with preprocessed text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\smrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\smrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\smrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                review_id             business_id  stars  \\\n",
      "0  KU_O5udG6zpxOg-VcAEodg  XQfwVwDr-v0ZS3_CbbE5Xw    3.0   \n",
      "1  BiTunyQ73aT9WBnpR9DZGw  7ATYjTIgM3jUlt4UM3IypQ    5.0   \n",
      "\n",
      "                                                text                 date  \\\n",
      "0  If you decide to eat here, just be aware it is...  2018-07-07 22:09:11   \n",
      "1  I've taken a lot of spin classes over the year...  2012-01-03 15:28:18   \n",
      "\n",
      "  preprocessed_text  \n",
      "0              None  \n",
      "1              None  \n",
      "Preprocessing completed! Saved the preprocessed Yelp sample.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    # Remove numbers and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Remove words with length ≤ 2\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "\n",
    "    # Convert list of tokens back to string\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect(\"yelp_data.db\")\n",
    "\n",
    "# Fetch a random sample of 100K reviews for fast preprocessing\n",
    "query = \"SELECT review_id, business_id,stars,date, text FROM review ORDER BY RANDOM() LIMIT 100000\"\n",
    "df_sample = pd.read_sql(query, conn)\n",
    "query1 = \"SELECT * FROM review LIMIT 2\"\n",
    "df_sample1 = pd.read_sql(query1, conn)\n",
    "conn.close()\n",
    "print(df_sample1)\n",
    "\n",
    "# Apply preprocessing\n",
    "df_sample[\"preprocessed_text\"] = df_sample[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# Save the preprocessed sample to a CSV or JSON file\n",
    "df_sample.to_csv(\"yelp_sample_preprocessed.csv\", index=False)\n",
    "df_sample.to_json(\"yelp_sample_preprocessed.json\", orient=\"records\", indent=4)\n",
    "\n",
    "print(\"Preprocessing completed! Saved the preprocessed Yelp sample.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
